{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0877d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import holoviews as hv\n",
    "\n",
    "from utig_radar_loading import file_util, stream_util, geo_util, opr_gps_file_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "tqdm.pandas()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "cache_dir = \"outputs/file_index.csv\"\n",
    "base_path = \"/kucresis/scratch/data/UTIG\"\n",
    "\n",
    "df_files = file_util.load_file_index_df(base_path, cache_dir, read_cache=use_cache)\n",
    "\n",
    "df_artifacts = file_util.create_artifacts_df(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just deal with UTIG2 for now\n",
    "df_artifacts = df_artifacts[df_artifacts['dataset'] == 'UTIG2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_by_transect(df_artifacts, streams):\n",
    "    \"\"\"\n",
    "    Group by transects (unique combinations of (prj, set, trn)) and pull out paths\n",
    "    to the desired data streams.\n",
    "\n",
    "    streams is a dictionary mapping names of data categories to a list of acceptable\n",
    "    stream types. For example:\n",
    "    { \"gps\": [\"GPSnc1\", \"GPSnc2\"],\n",
    "      \"radar\": [\"RADnh5\", \"RADnh6\"] }\n",
    "\n",
    "    The resulting dataframe will have two columns per entry in the streams dictionary:\n",
    "    <data category>_stream_type will contain the matched stream type and\n",
    "    <data category>_path will contain the path to the data file.\n",
    "\n",
    "    If multiple matching stream types are available, preference will be given to the\n",
    "    first stream type in the list. If no matching stream types are available, columns\n",
    "    will be filled with NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def agg_fn(group):\n",
    "        df = pd.DataFrame(index=[0])\n",
    "        \n",
    "        # Look for requested data streams\n",
    "        for data_category in streams.keys():\n",
    "            df[f\"{data_category}_stream_type\"] = np.nan\n",
    "            df[f\"{data_category}_path\"] = np.nan\n",
    "            for stream_type in streams[data_category]:\n",
    "                if stream_type in group['stream'].values:\n",
    "                    df[f\"{data_category}_stream_type\"] = stream_type\n",
    "                    df[f\"{data_category}_path\"] = group.loc[group['stream'] == stream_type, 'full_path'].values[0]\n",
    "                    break\n",
    "\n",
    "        # Add in any other unique keys\n",
    "        for k in group:\n",
    "            if k in ['full_path', 'stream', 'processing_level', 'processing_type']:\n",
    "                continue\n",
    "            \n",
    "            if len(group[k].unique()) == 1:\n",
    "                df[k] = group[k].values[0]\n",
    "\n",
    "        return df\n",
    "\n",
    "    df = df_artifacts.groupby(['prj', 'set', 'trn']).apply(agg_fn, include_groups=False)\n",
    "    df.index = df.index.droplevel(-1)\n",
    "    return df\n",
    "\n",
    "df_transects = arrange_by_transect(df_artifacts, {\n",
    "    \"gps\": [\"GPSnc1\"],\n",
    "    \"radar\": [\"RADnh5\"]\n",
    "})\n",
    "df_transects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transects = df_artifacts.groupby(['dataset', 'prj', 'set', 'trn']).agg(list).reset_index()\n",
    "# df_transects.head()\n",
    "\n",
    "# df_transects['has_radar'] = df_transects['artifact'].apply(lambda x: 'RAD' in str(x))\n",
    "# df_transects['has_gps'] = df_transects['artifact'].apply(lambda x: ('GPSnc1' in str(x)) or ('GPStp2' in str(x)))\n",
    "# #df_transects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_timestamp(transect):\n",
    "    # Iterate over stream data until we find one that has a valid context file\n",
    "    \n",
    "    fp = transect['gps_path']\n",
    "    ct_df = stream_util.load_ct_file(fp, read_csv_kwargs={'nrows': 1})\n",
    "    ct_df = stream_util.parse_CT(ct_df)\n",
    "\n",
    "    return ct_df.iloc[0]['TIMESTAMP']\n",
    "\n",
    "def get_end_timestamp(transect):\n",
    "    fp = transect['gps_path']\n",
    "    \n",
    "    # Read last few bytes and extract last line\n",
    "    with gzip.open(fp, 'rb') as f:\n",
    "        f.seek(-2, os.SEEK_END)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, os.SEEK_CUR)\n",
    "        last_line = f.readline().decode()\n",
    "    \n",
    "    # Load and parse just the last line\n",
    "    from io import StringIO\n",
    "    ct_columns = ['prj', 'set', 'trn', 'seq', 'clk_y', 'clk_n', 'clk_d', 'clk_h', 'clk_m', 'clk_s', 'clk_f', 'tim']\n",
    "    ct_df = pd.read_csv(StringIO(last_line), sep=r'\\s+', names=ct_columns, index_col=False)\n",
    "    ct_df = stream_util.parse_CT(ct_df)\n",
    "    return ct_df.iloc[0]['TIMESTAMP']\n",
    "\n",
    "def season_from_datetime(d):\n",
    "    if d.month >= 6:\n",
    "        return d.year\n",
    "    else:\n",
    "        return d.year - 1\n",
    "\n",
    "df_all_seasons = df_transects\n",
    "\n",
    "df_all_seasons['start_timestamp'] = df_all_seasons.apply(get_start_timestamp, axis=1)\n",
    "df_all_seasons['season'] = df_all_seasons['start_timestamp'].apply(season_from_datetime)\n",
    "df_all_seasons = df_all_seasons.sort_values('prj')\n",
    "#df_all_seasons.to_csv('tmp.csv')\n",
    "df_all_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following seasons were found in the dataset:\")\n",
    "print(df_all_seasons['season'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd740f8",
   "metadata": {},
   "source": [
    "### Select a single season to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63816801",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_year = 2018\n",
    "season_name = \"2018_Antarctica_BaslerJKB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "df_season = df_season.sort_values(by='start_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge segments\n",
    "\n",
    "last_segment_ct = stream_util.load_ct_file(df_season.iloc[0]['radar_path'])\n",
    "# Start tim difference threshold at 99th percentile\n",
    "tim_diff_threshold = np.percentile(np.diff(last_segment_ct['tim']), 99)\n",
    "print(f\"Using 'tim' difference threshold: {tim_diff_threshold} (10s of us)\")\n",
    "\n",
    "df_season['segment_path'] = \"\"\n",
    "df_season['segment_date_str'] = \"\"\n",
    "df_season['segment_number'] = -1\n",
    "current_segment_datestring = df_season.iloc[0]['start_timestamp'].strftime(\"%Y%m%d\")\n",
    "current_segment_idx = 1\n",
    "\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_date_str')] = current_segment_datestring\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_path')] = f\"{current_segment_datestring}_{current_segment_idx:02d}\"\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_number')] = current_segment_idx\n",
    "\n",
    "\n",
    "print(f\"Initial segment path is: {df_season.iloc[0]['segment_path']}\")\n",
    "\n",
    "for row_iloc in tqdm(range(1, len(df_season))):\n",
    "    curr_segment_ct = stream_util.load_ct_file(df_season.iloc[row_iloc]['radar_path'])\n",
    "    tim_delta_from_last = curr_segment_ct['tim'].iloc[0] - last_segment_ct['tim'].iloc[-1]\n",
    "\n",
    "    if np.abs(tim_delta_from_last) > tim_diff_threshold:\n",
    "        new_datestring = df_season.iloc[row_iloc]['start_timestamp'].strftime(\"%Y%m%d\")\n",
    "        if new_datestring == current_segment_datestring:\n",
    "            current_segment_idx += 1\n",
    "        else:\n",
    "            current_frame_idx = 1\n",
    "            current_segment_idx = 1\n",
    "            current_segment_datestring = new_datestring\n",
    "\n",
    "        print(f\"Segment path changed to {current_segment_datestring}_{current_segment_idx:02d}. Delta in 'tim' was {tim_delta_from_last}\")\n",
    "\n",
    "    df_season.iloc[row_iloc, df_season.columns.get_loc('segment_date_str')] = current_segment_datestring\n",
    "    df_season.iloc[row_iloc, df_season.columns.get_loc('segment_path')] = f\"{current_segment_datestring}_{current_segment_idx:02d}\"\n",
    "    df_season.iloc[row_iloc, df_season.columns.get_loc('segment_number')] = current_segment_idx\n",
    "\n",
    "    last_segment_ct = curr_segment_ct\n",
    "\n",
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gps_data(transects_df):\n",
    "    segment_dfs = []\n",
    "\n",
    "    for _, row in tqdm(transects_df.iterrows(), total=len(transects_df)):\n",
    "\n",
    "        f = row['gps_path']\n",
    "        \n",
    "        df = stream_util.load_gzipped_stream_file(f, debug=False, parse=True, parse_kwargs={'use_ct': True})\n",
    "\n",
    "        line_length_km = stream_util.calculate_track_distance_km(df)\n",
    "\n",
    "        _, _, line_length_m_shapely = geo_util.project_split_and_simplify(df['LON'].values, df['LAT'].values, calc_length=True, simplify_tolerance=100)\n",
    "\n",
    "        necessary_keys = ['prj', 'set', 'trn', 'clk_y', 'LAT', 'LON', 'TIMESTAMP']\n",
    "        for k in necessary_keys:\n",
    "            if k not in df:\n",
    "                df[k] = np.nan\n",
    "\n",
    "        df_sub = df[['prj', 'set', 'trn', 'clk_y', 'LAT', 'LON', 'TIMESTAMP']]\n",
    "\n",
    "        df_sub['segment_path'] = row['segment_path']\n",
    "\n",
    "        segment_dfs.append(df_sub)\n",
    "    return segment_dfs\n",
    "\n",
    "segment_dfs = load_gps_data(df_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for segment_path in df_season['segment_path'].unique():\n",
    "    dfs_list_tmp = [df for df in segment_dfs if df['segment_path'].iloc[0] == segment_path]\n",
    "    _, p = geo_util.create_path(dfs_list_tmp)\n",
    "    p = p.relabel(f\"Segment {segment_path}\")\n",
    "    paths.append(p)\n",
    "\n",
    "p = stream_util.create_antarctica_basemap() * hv.Overlay(paths)\n",
    "p = p.opts(aspect='equal', frame_width=500, frame_height=500, tools=['hover'])\n",
    "p.opts(title=season_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb230cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33149dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season.iloc[0:1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7780e",
   "metadata": {},
   "source": [
    "## Break segments into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a65379",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_distance = 50 # km\n",
    "\n",
    "frame_outputs = {}\n",
    "all_entries = []\n",
    "\n",
    "segment_paths = df_season['segment_path'].unique()\n",
    "for seg in segment_paths:\n",
    "    print(f\"Processing segment: {seg}\")\n",
    "    seg_df = df_season[df_season['segment_path'] == seg].sort_values('start_timestamp')\n",
    "    # Note: Should have already been sorted, but just in case\n",
    "\n",
    "    frame_idx = 1 # Frame index we're currently assigning\n",
    "    accumulated_km = 0 # Sum of line-km currently assigned to frame_idx\n",
    "    transect_iloc = 0 # Index of the current transect being processed\n",
    "\n",
    "    frame_outputs[seg] = {frame_idx: []}\n",
    "    last_x, last_y = None, None\n",
    "\n",
    "    for transect_iloc in range(len(seg_df)):\n",
    "        print(f\" -> Allocating transect {transect_iloc} {seg_df.index[transect_iloc]}\")\n",
    "\n",
    "        # Load the geometry of this transect\n",
    "        df = stream_util.load_gzipped_stream_file(\n",
    "            seg_df.iloc[transect_iloc]['gps_path'],\n",
    "            debug=False, parse=True, parse_kwargs={'use_ct': True}\n",
    "            )\n",
    "\n",
    "        x_proj, y_proj, line_length_m = geo_util.project_split_and_simplify(\n",
    "            df['LON'].values, df['LAT'].values, calc_length=True, simplify_tolerance=None)\n",
    "        \n",
    "        x_proj = x_proj[:-1]\n",
    "        y_proj = y_proj[:-1]\n",
    "\n",
    "        # Calculate the along-track distance, accounting for possible distance from the\n",
    "        # end of the last transect\n",
    "        deltas = np.sqrt(np.diff(x_proj)**2 + np.diff(y_proj)**2) / 1000  # Convert to km\n",
    "        if last_x:\n",
    "            deltas = np.insert(deltas, 0, np.sqrt((x_proj[0] - last_x)**2 + (y_proj[0] - last_y)**2) / 1000)\n",
    "        else:\n",
    "            deltas = np.insert(deltas, 0, 0)\n",
    "        dist = np.cumsum(deltas)\n",
    "        #print(f\"Transect total length is {dist[-1]} km\")\n",
    "        # print(x_proj)\n",
    "        # print(y_proj)\n",
    "        # print(dist)\n",
    "        # raise Exception(\"test\")\n",
    "\n",
    "        # Allocate parts of this transect to frames\n",
    "        transect_start_tim = df['tim'].iloc[0]\n",
    "        transect_start_idx = 0\n",
    "        while transect_start_tim < df['tim'].iloc[-1]:\n",
    "            # Find the 'tim' index that fits into the current segment\n",
    "            remaining_distance = break_distance - accumulated_km\n",
    "\n",
    "            dists_from_idx = np.maximum(0, dist - dist[transect_start_idx])\n",
    "            #print(f\"With transect_start_idx={transect_start_idx}, remaining distance in this transect is {dists_from_idx[-1]} km\")\n",
    "\n",
    "            break_idx = np.argmin(np.abs(dists_from_idx - remaining_distance))\n",
    "            break_tim = df['tim'].iloc[break_idx]\n",
    "\n",
    "            entry = seg_df.iloc[transect_iloc:transect_iloc+1].copy()\n",
    "            entry['gps_idx_start'] = transect_start_idx\n",
    "            entry['gps_idx_stop'] = break_idx\n",
    "            entry['tim_start'] = transect_start_tim\n",
    "            entry['tim_stop'] = break_tim\n",
    "            entry['frame_number'] = frame_idx\n",
    "\n",
    "            all_entries.append(entry)\n",
    "\n",
    "            # Add an entry to this frame and update distance\n",
    "            frame_outputs[seg][frame_idx].append(entry)\n",
    "            accumulated_km += dist[break_idx] - dist[transect_start_idx]\n",
    "            print(f\"   -> Assigned indices {transect_start_idx} to {break_idx} (distance {dist[break_idx] - dist[transect_start_idx]} km) to frame {frame_idx}, now at {accumulated_km} km\")\n",
    "\n",
    "            # Move transect start index\n",
    "            transect_start_idx = break_idx\n",
    "            transect_start_tim = break_tim\n",
    "\n",
    "            # Check if the frame is full\n",
    "            if accumulated_km >= 0.98*break_distance:\n",
    "                print(f\"    Frame {frame_idx} is full with {accumulated_km} km\")\n",
    "                frame_idx += 1\n",
    "                accumulated_km = 0\n",
    "                frame_outputs[seg][frame_idx] = []\n",
    "            \n",
    "\n",
    "        last_x, last_y = x_proj[-1], y_proj[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8efc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_plan_df = pd.concat(all_entries).reset_index().set_index(['segment_date_str', 'segment_number', 'frame_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment_gps_file(x):\n",
    "    x = x.reset_index()\n",
    "    print(f\"{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}\")\n",
    "    gps_paths = list(x['gps_path'].unique())\n",
    "    output_path = f\"outputs/gps/{season_name}/gps_{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}.mat\"\n",
    "\n",
    "    opr_gps_file_generation.generate_gps_file(gps_paths, output_path, format='hdf5')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "frames_plan_df.groupby(['segment_date_str', 'segment_number']).apply(make_segment_gps_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f48e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we actually wrote an HDF5 file\n",
    "\n",
    "import h5py\n",
    "\n",
    "fn = '/kucresis/scratch/tteisberg_sta/scripts/python/utig_radar_loading/outputs/gps/2018_Antarctica_BaslerJKB/gps_20190114_1.mat'\n",
    "\n",
    "with h5py.File(fn, 'r') as f:\n",
    "    print(\"Keys: %s\" % f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-load-utig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
